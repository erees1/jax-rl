{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "harmful-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import train, test, demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sporting-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flying-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emotional-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "underlying-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing model with layers [4, 32, 32, 32, 2]\n",
      "Warmup: Episode 0, Total Steps 21, Reward 21.0, Epsilon 1.0000\n",
      "Warmup: Episode 1, Total Steps 66, Reward 45.0, Epsilon 1.0000\n",
      "Warmup: Episode 2, Total Steps 90, Reward 24.0, Epsilon 1.0000\n",
      "Warmup: Episode 3, Total Steps 105, Reward 15.0, Epsilon 1.0000\n",
      "Warmup: Episode 4, Total Steps 118, Reward 13.0, Epsilon 1.0000\n",
      "Warmup: Episode 5, Total Steps 143, Reward 25.0, Epsilon 1.0000\n",
      "Warmup: Episode 6, Total Steps 153, Reward 10.0, Epsilon 1.0000\n",
      "Warmup: Episode 7, Total Steps 180, Reward 27.0, Epsilon 1.0000\n",
      "Warmup: Episode 8, Total Steps 210, Reward 30.0, Epsilon 1.0000\n",
      "Warmup: Episode 9, Total Steps 230, Reward 20.0, Epsilon 1.0000\n",
      "Training: Episode 10, Total Steps 271, Reward 41.0, Epsilon 0.9952, Loss 1.399376\n",
      "Training: Episode 11, Total Steps 286, Reward 15.0, Epsilon 0.9897, Loss 1.410757\n",
      "Training: Episode 12, Total Steps 313, Reward 27.0, Epsilon 0.9799, Loss 1.434662\n",
      "Training: Episode 13, Total Steps 328, Reward 15.0, Epsilon 0.9745, Loss 1.464131\n",
      "Training: Episode 14, Total Steps 343, Reward 15.0, Epsilon 0.9692, Loss 1.508668\n",
      "Training: Episode 15, Total Steps 360, Reward 17.0, Epsilon 0.9632, Loss 1.565687\n",
      "Training: Episode 16, Total Steps 392, Reward 32.0, Epsilon 0.9520, Loss 1.655354\n",
      "Training: Episode 17, Total Steps 411, Reward 19.0, Epsilon 0.9454, Loss 1.739030\n",
      "Training: Episode 18, Total Steps 475, Reward 64.0, Epsilon 0.9237, Loss 1.916275\n",
      "Training: Episode 19, Total Steps 487, Reward 12.0, Epsilon 0.9197, Loss 2.232056\n",
      "Training: Episode 20, Total Steps 499, Reward 12.0, Epsilon 0.9157, Loss 2.546183\n",
      "Training: Episode 21, Total Steps 527, Reward 28.0, Epsilon 0.9065, Loss 2.970939\n",
      "Training: Episode 22, Total Steps 545, Reward 18.0, Epsilon 0.9006, Loss 3.921429\n",
      "Training: Episode 23, Total Steps 559, Reward 14.0, Epsilon 0.8961, Loss 4.945572\n",
      "Training: Episode 24, Total Steps 581, Reward 22.0, Epsilon 0.8891, Loss 6.889853\n",
      "Training: Episode 25, Total Steps 591, Reward 10.0, Epsilon 0.8859, Loss 9.902108\n",
      "Training: Episode 26, Total Steps 602, Reward 11.0, Epsilon 0.8824, Loss 11.074133\n",
      "Training: Episode 27, Total Steps 620, Reward 18.0, Epsilon 0.8768, Loss 16.162661\n",
      "Training: Episode 28, Total Steps 639, Reward 19.0, Epsilon 0.8709, Loss 24.220762\n",
      "Training: Episode 29, Total Steps 673, Reward 34.0, Epsilon 0.8604, Loss 52.858204\n",
      "Training: Episode 30, Total Steps 728, Reward 55.0, Epsilon 0.8438, Loss 256.494080\n",
      "Training: Episode 31, Total Steps 751, Reward 23.0, Epsilon 0.8370, Loss 812.049988\n",
      "Training: Episode 32, Total Steps 777, Reward 26.0, Epsilon 0.8294, Loss 1384.758911\n",
      "Training: Episode 33, Total Steps 798, Reward 21.0, Epsilon 0.8233, Loss 2805.531494\n",
      "Training: Episode 34, Total Steps 821, Reward 23.0, Epsilon 0.8167, Loss 4171.062988\n",
      "Training: Episode 35, Total Steps 849, Reward 28.0, Epsilon 0.8088, Loss 8120.130371\n",
      "Training: Episode 36, Total Steps 870, Reward 21.0, Epsilon 0.8029, Loss 11733.076172\n",
      "Training: Episode 37, Total Steps 883, Reward 13.0, Epsilon 0.7993, Loss 16675.683594\n",
      "Training: Episode 38, Total Steps 904, Reward 21.0, Epsilon 0.7935, Loss 21199.685547\n",
      "Training: Episode 39, Total Steps 941, Reward 37.0, Epsilon 0.7835, Loss 33038.335938\n",
      "Training: Episode 40, Total Steps 970, Reward 29.0, Epsilon 0.7757, Loss 19616.279297\n",
      "Training: Episode 41, Total Steps 981, Reward 11.0, Epsilon 0.7728, Loss 9994.396484\n",
      "Training: Episode 42, Total Steps 1012, Reward 31.0, Epsilon 0.7646, Loss 3990.721191\n",
      "Training: Episode 43, Total Steps 1037, Reward 25.0, Epsilon 0.7582, Loss 1178.806274\n",
      "Training: Episode 44, Total Steps 1059, Reward 22.0, Epsilon 0.7525, Loss 493.983032\n",
      "Training: Episode 45, Total Steps 1069, Reward 10.0, Epsilon 0.7500, Loss 575.173706\n",
      "Training: Episode 46, Total Steps 1105, Reward 36.0, Epsilon 0.7409, Loss 743.997742\n",
      "Training: Episode 47, Total Steps 1129, Reward 24.0, Epsilon 0.7349, Loss 830.145264\n",
      "Training: Episode 48, Total Steps 1142, Reward 13.0, Epsilon 0.7317, Loss 676.900146\n",
      "Training: Episode 49, Total Steps 1188, Reward 46.0, Epsilon 0.7205, Loss 812.054016\n",
      "Training: Episode 50, Total Steps 1231, Reward 43.0, Epsilon 0.7103, Loss 1117.497314\n",
      "Training: Episode 51, Total Steps 1270, Reward 39.0, Epsilon 0.7012, Loss 1569.151001\n",
      "Training: Episode 52, Total Steps 1368, Reward 98.0, Epsilon 0.6790, Loss 1740.208374\n",
      "Training: Episode 53, Total Steps 1386, Reward 18.0, Epsilon 0.6750, Loss 2155.345459\n",
      "Training: Episode 54, Total Steps 1467, Reward 81.0, Epsilon 0.6576, Loss 2250.020752\n",
      "Training: Episode 55, Total Steps 1525, Reward 58.0, Epsilon 0.6455, Loss 2887.755859\n",
      "Training: Episode 56, Total Steps 1542, Reward 17.0, Epsilon 0.6420, Loss 3473.044678\n",
      "Training: Episode 57, Total Steps 1571, Reward 29.0, Epsilon 0.6361, Loss 3047.396729\n",
      "Training: Episode 58, Total Steps 1651, Reward 80.0, Epsilon 0.6203, Loss 3233.596191\n",
      "Training: Episode 59, Total Steps 1666, Reward 15.0, Epsilon 0.6174, Loss 3962.297119\n",
      "Training: Episode 60, Total Steps 1753, Reward 87.0, Epsilon 0.6009, Loss 3977.976562\n",
      "Training: Episode 61, Total Steps 1845, Reward 92.0, Epsilon 0.5842, Loss 3648.627686\n",
      "Training: Episode 62, Total Steps 1952, Reward 107.0, Epsilon 0.5657, Loss 4352.435547\n",
      "Training: Episode 63, Total Steps 2049, Reward 97.0, Epsilon 0.5497, Loss 5141.641113\n",
      "Training: Episode 64, Total Steps 2174, Reward 125.0, Epsilon 0.5300, Loss 6529.499023\n",
      "Training: Episode 65, Total Steps 2258, Reward 84.0, Epsilon 0.5175, Loss 6546.004395\n",
      "Training: Episode 66, Total Steps 2283, Reward 25.0, Epsilon 0.5138, Loss 6321.129395\n",
      "Training: Episode 67, Total Steps 2418, Reward 135.0, Epsilon 0.4949, Loss 6970.758301\n",
      "Training: Episode 68, Total Steps 2549, Reward 131.0, Epsilon 0.4775, Loss 6838.240723\n",
      "Training: Episode 69, Total Steps 2667, Reward 118.0, Epsilon 0.4628, Loss 6089.923340\n",
      "Training: Episode 70, Total Steps 2687, Reward 20.0, Epsilon 0.4604, Loss 4908.020508\n",
      "Training: Episode 71, Total Steps 2734, Reward 47.0, Epsilon 0.4548, Loss 3738.495361\n",
      "Training: Episode 72, Total Steps 2844, Reward 110.0, Epsilon 0.4422, Loss 2859.854492\n",
      "Training: Episode 73, Total Steps 3038, Reward 194.0, Epsilon 0.4214, Loss 3646.685547\n",
      "Training: Episode 74, Total Steps 3106, Reward 68.0, Epsilon 0.4146, Loss 4151.760742\n",
      "Training: Episode 75, Total Steps 3306, Reward 200.0, Epsilon 0.3956, Loss 5289.594238\n",
      "Training: Episode 76, Total Steps 3659, Reward 353.0, Epsilon 0.3662, Loss 16357.156250\n",
      "Training: Episode 77, Total Steps 3936, Reward 277.0, Epsilon 0.3462, Loss 88398.445312\n",
      "Training: Episode 78, Total Steps 4154, Reward 218.0, Epsilon 0.3322, Loss 134971.750000\n",
      "Training: Episode 79, Total Steps 4386, Reward 232.0, Epsilon 0.3188, Loss 160290.062500\n",
      "Training: Episode 80, Total Steps 4563, Reward 177.0, Epsilon 0.3094, Loss 198324.843750\n",
      "Training: Episode 81, Total Steps 4764, Reward 201.0, Epsilon 0.2997, Loss 179962.281250\n",
      "Training: Episode 82, Total Steps 4822, Reward 58.0, Epsilon 0.2971, Loss 263328.593750\n",
      "Training: Episode 83, Total Steps 4976, Reward 154.0, Epsilon 0.2904, Loss 242032.703125\n",
      "Training: Episode 84, Total Steps 5227, Reward 251.0, Epsilon 0.2805, Loss 318674.812500\n",
      "Training: Episode 85, Total Steps 5417, Reward 190.0, Epsilon 0.2737, Loss 304515.781250\n",
      "Training: Episode 86, Total Steps 5603, Reward 186.0, Epsilon 0.2677, Loss 270940.562500\n",
      "Training: Episode 87, Total Steps 5800, Reward 197.0, Epsilon 0.2618, Loss 134748.781250\n",
      "Training: Episode 88, Total Steps 5919, Reward 119.0, Epsilon 0.2585, Loss 76495.484375\n",
      "Training: Episode 89, Total Steps 6094, Reward 175.0, Epsilon 0.2539, Loss 61928.687500\n",
      "Training: Episode 90, Total Steps 6257, Reward 163.0, Epsilon 0.2500, Loss 60373.558594\n",
      "Training: Episode 91, Total Steps 6365, Reward 108.0, Epsilon 0.2476, Loss 43895.976562\n",
      "Training: Episode 92, Total Steps 6561, Reward 196.0, Epsilon 0.2435, Loss 110883.343750\n",
      "Training: Episode 93, Total Steps 6681, Reward 120.0, Epsilon 0.2411, Loss 53220.699219\n",
      "Training: Episode 94, Total Steps 6813, Reward 132.0, Epsilon 0.2387, Loss 20065.955078\n",
      "Training: Episode 95, Total Steps 7012, Reward 199.0, Epsilon 0.2353, Loss 11182.472656\n",
      "Training: Episode 96, Total Steps 7170, Reward 158.0, Epsilon 0.2328, Loss 15386.612305\n",
      "Training: Episode 97, Total Steps 7430, Reward 260.0, Epsilon 0.2291, Loss 8352.702148\n",
      "Training: Episode 98, Total Steps 7588, Reward 158.0, Epsilon 0.2270, Loss 6733.216797\n",
      "Training: Episode 99, Total Steps 7701, Reward 113.0, Epsilon 0.2257, Loss 6507.691406\n",
      "Training: Episode 100, Total Steps 7759, Reward 58.0, Epsilon 0.2250, Loss 5549.674805\n",
      "Training: Episode 101, Total Steps 7869, Reward 110.0, Epsilon 0.2238, Loss 7102.393555\n",
      "Training: Episode 102, Total Steps 7930, Reward 61.0, Epsilon 0.2231, Loss 5410.177246\n",
      "Training: Episode 103, Total Steps 8020, Reward 90.0, Epsilon 0.2221, Loss 6457.511230\n",
      "Training: Episode 104, Total Steps 8069, Reward 49.0, Epsilon 0.2217, Loss 5174.059570\n",
      "Training: Episode 105, Total Steps 8156, Reward 87.0, Epsilon 0.2208, Loss 6574.691895\n",
      "Training: Episode 106, Total Steps 8234, Reward 78.0, Epsilon 0.2201, Loss 7352.554688\n",
      "Training: Episode 107, Total Steps 8313, Reward 79.0, Epsilon 0.2193, Loss 6660.254883\n",
      "Training: Episode 108, Total Steps 8424, Reward 111.0, Epsilon 0.2184, Loss 5694.215332\n",
      "Training: Episode 109, Total Steps 8457, Reward 33.0, Epsilon 0.2181, Loss 3903.673340\n",
      "Training: Episode 110, Total Steps 8704, Reward 247.0, Epsilon 0.2161, Loss 89271.187500\n",
      "Training: Episode 111, Total Steps 9204, Reward 500.0, Epsilon 0.2128, Loss 104785.765625\n",
      "Training: Episode 112, Total Steps 9311, Reward 107.0, Epsilon 0.2122, Loss 6232.668945\n",
      "Training: Episode 113, Total Steps 9343, Reward 32.0, Epsilon 0.2120, Loss 12735.747070\n",
      "Training: Episode 114, Total Steps 9618, Reward 275.0, Epsilon 0.2106, Loss 68934.414062\n",
      "Training: Episode 115, Total Steps 9810, Reward 192.0, Epsilon 0.2097, Loss 16950.283203\n",
      "Training: Episode 116, Total Steps 9913, Reward 103.0, Epsilon 0.2092, Loss 21540.242188\n",
      "Training: Episode 117, Total Steps 10043, Reward 130.0, Epsilon 0.2087, Loss 16348.334961\n",
      "Training: Episode 118, Total Steps 10175, Reward 132.0, Epsilon 0.2082, Loss 14550.671875\n",
      "Training: Episode 119, Total Steps 10302, Reward 127.0, Epsilon 0.2077, Loss 93702.171875\n"
     ]
    }
   ],
   "source": [
    "rewards, losses, agent = train(env, seed=1, n_layers=3, batch_size=256, train_eps=120, warm_up_steps=256, discount_factor=0.99, epsilon_hlife=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "substantial-verification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<agent.Agent at 0x7fb2be462040>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aggressive-cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: Episode 0, Total Steps 100, Reward 100.0\n",
      "Testing: Episode 1, Total Steps 329, Reward 229.0\n",
      "Testing: Episode 2, Total Steps 578, Reward 249.0\n",
      "Testing: Episode 3, Total Steps 836, Reward 258.0\n",
      "Testing: Episode 4, Total Steps 1095, Reward 259.0\n",
      "Testing: Episode 5, Total Steps 1340, Reward 245.0\n",
      "Testing: Episode 6, Total Steps 1561, Reward 221.0\n",
      "Testing: Episode 7, Total Steps 1784, Reward 223.0\n",
      "Testing: Episode 8, Total Steps 2008, Reward 224.0\n",
      "Testing: Episode 9, Total Steps 2247, Reward 239.0\n",
      "Testing: Episode 10, Total Steps 2486, Reward 239.0\n",
      "Testing: Episode 11, Total Steps 2709, Reward 223.0\n",
      "Testing: Episode 12, Total Steps 2896, Reward 187.0\n",
      "Testing: Episode 13, Total Steps 3132, Reward 236.0\n",
      "Testing: Episode 14, Total Steps 3252, Reward 120.0\n",
      "Testing: Episode 15, Total Steps 3405, Reward 153.0\n",
      "Testing: Episode 16, Total Steps 3636, Reward 231.0\n",
      "Testing: Episode 17, Total Steps 3869, Reward 233.0\n",
      "Testing: Episode 18, Total Steps 4019, Reward 150.0\n",
      "Testing: Episode 19, Total Steps 4204, Reward 185.0\n",
      "Testing: Average reward over 20 episodes 210.200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[100.0,\n",
       " 229.0,\n",
       " 249.0,\n",
       " 258.0,\n",
       " 259.0,\n",
       " 245.0,\n",
       " 221.0,\n",
       " 223.0,\n",
       " 224.0,\n",
       " 239.0,\n",
       " 239.0,\n",
       " 223.0,\n",
       " 187.0,\n",
       " 236.0,\n",
       " 120.0,\n",
       " 153.0,\n",
       " 231.0,\n",
       " 233.0,\n",
       " 150.0,\n",
       " 185.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(env, agent, test_eps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "applicable-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('agent.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alive-relative",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing model with layers [4, 32, 32, 32, 2]\n",
      "Successfully loaded model from agent.npz\n"
     ]
    }
   ],
   "source": [
    "new_agent = Agent.load('agent.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "representative-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "endless-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.random.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "powerful-mechanics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2109.4075, 2485.0195], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_agent.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abstract-electric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2109.4075, 2485.0195], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "french-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "convertible-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "empirical-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.random.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "handled-lender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.act(key, obs, 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bearing-change",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_agent.act(key, obs, 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sixth-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ongoing-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import test, demo\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polyphonic-shame",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing model with layers [4, 32, 32, 32, 2]\n",
      "Initializing model with layers [4, 32, 32, 32, 2]\n",
      "INFO:root:Successfully loaded model from agent.npz\n",
      "Successfully loaded model from agent.npz\n"
     ]
    }
   ],
   "source": [
    "new_agent = Agent.load('agent.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complex-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Testing: Episode 0, Total Steps 242, Reward 242.0\n",
      "Testing: Episode 0, Total Steps 242, Reward 242.0\n",
      "INFO:root:Testing: Episode 1, Total Steps 473, Reward 231.0\n",
      "Testing: Episode 1, Total Steps 473, Reward 231.0\n",
      "INFO:root:Testing: Episode 2, Total Steps 690, Reward 217.0\n",
      "Testing: Episode 2, Total Steps 690, Reward 217.0\n",
      "INFO:root:Testing: Episode 3, Total Steps 918, Reward 228.0\n",
      "Testing: Episode 3, Total Steps 918, Reward 228.0\n",
      "INFO:root:Testing: Episode 4, Total Steps 1158, Reward 240.0\n",
      "Testing: Episode 4, Total Steps 1158, Reward 240.0\n",
      "INFO:root:Testing: Episode 5, Total Steps 1390, Reward 232.0\n",
      "Testing: Episode 5, Total Steps 1390, Reward 232.0\n",
      "INFO:root:Testing: Episode 6, Total Steps 1617, Reward 227.0\n",
      "Testing: Episode 6, Total Steps 1617, Reward 227.0\n",
      "INFO:root:Testing: Episode 7, Total Steps 1839, Reward 222.0\n",
      "Testing: Episode 7, Total Steps 1839, Reward 222.0\n",
      "INFO:root:Testing: Episode 8, Total Steps 2039, Reward 200.0\n",
      "Testing: Episode 8, Total Steps 2039, Reward 200.0\n",
      "INFO:root:Testing: Episode 9, Total Steps 2290, Reward 251.0\n",
      "Testing: Episode 9, Total Steps 2290, Reward 251.0\n",
      "INFO:root:Testing: Episode 10, Total Steps 2529, Reward 239.0\n",
      "Testing: Episode 10, Total Steps 2529, Reward 239.0\n",
      "INFO:root:Testing: Episode 11, Total Steps 2753, Reward 224.0\n",
      "Testing: Episode 11, Total Steps 2753, Reward 224.0\n",
      "INFO:root:Testing: Episode 12, Total Steps 3005, Reward 252.0\n",
      "Testing: Episode 12, Total Steps 3005, Reward 252.0\n",
      "INFO:root:Testing: Episode 13, Total Steps 3240, Reward 235.0\n",
      "Testing: Episode 13, Total Steps 3240, Reward 235.0\n",
      "INFO:root:Testing: Episode 14, Total Steps 3460, Reward 220.0\n",
      "Testing: Episode 14, Total Steps 3460, Reward 220.0\n",
      "INFO:root:Testing: Episode 15, Total Steps 3689, Reward 229.0\n",
      "Testing: Episode 15, Total Steps 3689, Reward 229.0\n",
      "INFO:root:Testing: Episode 16, Total Steps 3941, Reward 252.0\n",
      "Testing: Episode 16, Total Steps 3941, Reward 252.0\n",
      "INFO:root:Testing: Episode 17, Total Steps 4168, Reward 227.0\n",
      "Testing: Episode 17, Total Steps 4168, Reward 227.0\n",
      "INFO:root:Testing: Episode 18, Total Steps 4401, Reward 233.0\n",
      "Testing: Episode 18, Total Steps 4401, Reward 233.0\n",
      "INFO:root:Testing: Episode 19, Total Steps 4623, Reward 222.0\n",
      "Testing: Episode 19, Total Steps 4623, Reward 222.0\n",
      "INFO:root:Testing: Average reward over 20 episodes 231.150\n",
      "Testing: Average reward over 20 episodes 231.150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[242.0,\n",
       " 231.0,\n",
       " 217.0,\n",
       " 228.0,\n",
       " 240.0,\n",
       " 232.0,\n",
       " 227.0,\n",
       " 222.0,\n",
       " 200.0,\n",
       " 251.0,\n",
       " 239.0,\n",
       " 224.0,\n",
       " 252.0,\n",
       " 235.0,\n",
       " 220.0,\n",
       " 229.0,\n",
       " 252.0,\n",
       " 227.0,\n",
       " 233.0,\n",
       " 222.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(env, new_agent, test_eps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "transsexual-directive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for (w,b), (w2, b2) in zip(new_agent.params, agent.params):\n",
    "    print(w-w2)\n",
    "    print(b-b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "orange-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "indirect-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "checked-invitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sixth-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "continuous-colonial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "trained-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "olive-correlation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "collective-occupation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo(env, agent, test_eps=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-dqn",
   "language": "python",
   "name": "jax-dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
